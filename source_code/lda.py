"""
Santa Clara University

COEN 240 - Machine Learning

Final Project

Quan Bach
Anh Truong



By running thi file: 
    - a table of n topics will be displayed in output (n = 8 in this script and it can be changed)
    - two html files will be saved to a folder named LDA under the name lda-bow.html and lda-tfidf.html
    - two html files is an interactive visualization of the lda models trained by bags of words and tf-idf
    
    
*** NOTICE ***: this script will dowload the 20 news group dataset from sklearn and perform processing directly here before the lda models. 
Therefore, it does not used the vocabularies generated by other files previously. However, the content should be the same. 

Dependencies: 
    - gensim
    - pyLDAvis
    

"""
import os
import gensim
import pyLDAvis.gensim
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from sklearn.datasets import fetch_20newsgroups
from gensim import models



# Tokenize and lemmatize

def lemmatize_stemming(text):
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))
def preprocess(text):
    result=[]
    for token in gensim.utils.simple_preprocess(text) :
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(lemmatize_stemming(token))
            
    return result


if __name__ == '__main__':
    
    if not os.path.exists('./LDA/'):
           os.makedirs('./LDA/')
    
    stemmer = SnowballStemmer("english")
    # load data 
    newsgroups_train = fetch_20newsgroups(subset='train', shuffle = True)    
    
    # initialize empty list to hold all docs 
    processed_docs = []
    
    # process the docs
    
    for doc in newsgroups_train.data:
        processed_docs.append(preprocess(doc))
            
    dictionary = gensim.corpora.Dictionary(processed_docs)


    
    # Remove very rare and very common words:
    # -words appearing less than 15 times
    # -words appearing in more than 10% of all documents
    dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)
    
    # Create a bag or word for each document 
    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]


    # train LDA model with bags of words 
    lda_model_bow =  gensim.models.LdaMulticore(bow_corpus, 
                                   num_topics = 8, 
                                   id2word = dictionary,                                    
                                   passes = 10,
                                   workers = 2)


    print('**** LDA MODEL USING BAGS OF WORDS *****')
    # Print out the words in each topic and its weight 
    for idx, topic in lda_model_bow.print_topics(-1):
        print("Topic: {} \nWords: {}".format(idx, topic ))
        print()
        
    print('*' * 20)
    print()
    
    lda_bow_vis = pyLDAvis.gensim.prepare(lda_model_bow, bow_corpus, dictionary=lda_model_bow.id2word)  
        
    # save visualization to html
    pyLDAvis.save_html(lda_bow_vis, './LDA/lda-bow.html')
    
    
    # performing tf-idf from bags of words
    
    tfidf = models.TfidfModel(bow_corpus)
    tfidf_corpus = tfidf[bow_corpus]
    
    lda_model_tfidf =  gensim.models.LdaMulticore(tfidf_corpus, 
                                   num_topics = 8, 
                                   id2word = dictionary,                                    
                                   passes = 10,
                                   workers = 2)
    
    print('**** LDA MODEL USING TF-IDF  *****')
    # Print out the words in each topic and its weight 
    for idx, topic in lda_model_tfidf.print_topics(-1):
        print("Topic: {} \nWords: {}".format(idx, topic ))
        print()
        
    print('*' * 20)
    print()
    
    lda_tfidf_vis = pyLDAvis.gensim.prepare(lda_model_tfidf, tfidf_corpus, dictionary=lda_model_tfidf.id2word)  
        
    # save visualization to html
    pyLDAvis.save_html(lda_tfidf_vis, './LDA/lda-tfidf.html')